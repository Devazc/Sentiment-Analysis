{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Logistic Regression Model With Personal Data\n",
    "## Includes Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef81321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Context Variables and Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393be6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder\\\n",
    "            .master(\"local[16]\")\\\n",
    "            .appName(\"TOT\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../resources/training_noemoticon.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n",
    "\n",
    "df = spark1.read.csv(path,\n",
    "                     inferSchema=True,\n",
    "                     header=False,\n",
    "                     schema=schema)\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inheriting Class from PySpark Transformer in order to be usable in Pipeline or with a classname.transform(dataset)\n",
    "\n",
    "__init__ :\n",
    "initialize the column names, the regex allowing to filter the URLs and other Unwanted Characters\n",
    "\n",
    "\n",
    " _transform:\n",
    " apply the transformation on the given Dataset\n",
    "\n",
    " Attention : The Process is not well optimized and will take longer to complete\n",
    " but it will improve the accuracy\n",
    "\"\"\"\n",
    "class WordFormatter(Transformer):\n",
    "    def __init__(self, *, inputCol, outputCol):\n",
    "\n",
    "        super(WordFormatter, self).__init__()\n",
    "        self.regpat = re.compile(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b')\n",
    "        self.ponc = [',', '.', '?', '-']\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def stopw(self, wt):\n",
    "\n",
    "        filtered_sentence = []\n",
    "\n",
    "        # pour tout les mots\n",
    "        for w in wt[\"filtered\"]:\n",
    "            wrd = w.strip().lower()\n",
    "\n",
    "            # on vérifie si la ponctuation non désirée est présente, on l'enlève si oui\n",
    "            for char in self.ponc:\n",
    "                wrd = wrd.replace(char, '')\n",
    "\n",
    "            # on ne garde pas les mots avec un URL, mot vide, mot ' ' ou des mots avec '@'\n",
    "            if not re.fullmatch(self.regpat, wrd) and wrd != ' ' and wrd != '' and '@' not in wrd:\n",
    "                filtered_sentence.append(wrd)\n",
    "\n",
    "        # renvoie la ligne telle qu'elle était mais avec notre liste en plus\n",
    "        return wt[0], wt[1], wt[2], wt[3], wt[4], wt[5], wt[6], filtered_sentence\n",
    "\n",
    "    def _transform(self, dtf: DataFrame) -> DataFrame:\n",
    "\n",
    "        # applique le stop words de PySpark\n",
    "        rem = StopWordsRemover(inputCol=self.inputCol, outputCol=\"filtered\")\n",
    "        ndtf = rem.transform(dtf)\n",
    "\n",
    "        # applique notre fonction en plus pour mieux filtrer\n",
    "        rdd = ndtf.rdd.map(lambda x: self.stopw(x))\n",
    "        ndtf = rdd.toDF()\n",
    "\n",
    "        # remets les noms de colonne de départ\n",
    "        official_col = ['target', 'id', 'date', 'query', 'author', 'tweet', self.inputCol, self.outputCol]\n",
    "\n",
    "        for old, new in zip(ndtf.columns, official_col):\n",
    "            ndtf = ndtf.withColumnRenamed(old, new)\n",
    "\n",
    "        return ndtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_set, test_set) = df.randomSplit([0.80, 0.20], seed = 2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9be6ad42",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40891d71",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (Custom Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8356be2",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (Dafault Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab87e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\")\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, cv, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fca8ad0",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (Custom Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, cv, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6227e0f9",
   "metadata": {},
   "source": [
    "# CountVectorizer + NGram + ChisQSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=[\"tweet\",\"target\"], n=3):\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")]\n",
    "    wordform = [WordFormatter(inputCol=\"tk\", outputCol=\"words\")]\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    \n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    \n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    \n",
    "    lr = [LogisticRegression()]\n",
    "    \n",
    "    return Pipeline(stages=tokenizer + wordform + ngrams + cv + idf + assembler + label_stringIdx + selector + lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = build_trigrams().fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
